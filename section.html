<section class="section is-light" id="results">
    <div class="columns is-centered">
      <div class="column has-text-centered">
        <div class="title-container">
          <h1 class="main-title">STAMP:</h1>
          <h2 class="subtitle">SCALABLE TASK AND MODEL-AGNOSTIC COLLABORATIVE PERCEPTION</h2>
        </div>
        
       
  
       
      </div>
    </div>
  

    <div class="container is-widescreen">
      <div class="container is-widescreen">
        <div class="columns is-centered">
          <div class="sub-sub-title section is-flex is-align-items-center is-justify-content-center is-fullwidth"  style="white-space: nowrap;">
            <p class="is-size-4">Example 1</p>
          </div>
          <div class="column is-half">
            <div class="sub-sub-title">
              <span style="color: #7e56d0;">Before</span> Collaborative Feature Ailgnment (CFA)
            </div>
            <div class="video-container">
              <video id="video1" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
                <source src="static/videos/adaptiveCAV_identity_276.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </div>
          <div class="column is-half">
            <div class="sub-sub-title">
              <span style="color: #7e56d0;">After</span> Collaborative Feature Ailgnment (CFA)
            </div>
            <div class="video-container">
              <video id="video2" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
                <source src="static/videos/adaptiveCAV_276.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              </div>
          </div>
        </div>
      </div>

      <br>
      <br>
      <div class="container is-widescreen">
        <div class="container is-widescreen">
          <div class="columns is-centered">
            <div class="sub-sub-title section is-flex is-align-items-center is-justify-content-center is-fullwidth"  style="white-space: nowrap;">
              <p class="is-size-4">Example 2</p>
            </div>
            <div class="column is-half">
              <div class="sub-sub-title">
                <span style="color: #7e56d0;">Before</span> Collaborative Feature Ailgnment (CFA)
              </div>
              <div class="video-container">
                <video id="video3" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
                  <source src="static/videos/adaptiveCAV_identity_00536.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
            <div class="column is-half">
              <div class="sub-sub-title">
                <span style="color: #7e56d0;">After</span> Collaborative Feature Ailgnment (CFA)
              </div>
                <div class="video-container">
                <video id="video4" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
                  <source src="static/videos/adaptiveCAV_00536.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
            </div>
          </div>
        </div>

      






      <!-- <div class="columns is-centered">
        <div class="column is-half">
          <div class="video-container">
            <video id="video1" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
              <source src="static/videos/adaptiveCAV_identity_276.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
        <div class="column is-half">
          <div class="video-container">
            <video id="video2" controls autoplay loop muted preload="auto" style="border-radius: 15px;" poster="">
              <source src="static/videos/adaptiveCAV_276.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div>
      <br>
      <div class="buttons is-centered mb-4">
        <button class="button is-medium is-rounded dataset-btn active" data-dataset="d1">Demo 1</button>
        <button class="button is-medium is-rounded dataset-btn" data-dataset="d2">Demo 2</button>
        <button class="button is-medium is-rounded dataset-btn" data-dataset="d3">Demo 3</button>
        <button class="button is-medium is-rounded dataset-btn" data-dataset="d4">Demo 4</button>
      </div> -->
<!--   
      <div id="scene-controls" class="columns is-centered is-mobile mb-4">
        <div class="column is-narrow" style="width: 160px;">
          <div class="field">
            <label class="label">Scene:</label>
            <div class="control">
              <div class="select">
                <select id="scene-select"></select>
              </div>
            </div>
          </div>
        </div>
        <div class="column is-narrow" style="width: 200px;">
          <div class="field">
            <label class="label">Cameras:</label>
            <div class="control">
              <div class="select">
                <select id="cameras-select"></select>
              </div>
            </div>
          </div>
        </div>
      </div>
  
      <div id="nuplan-message" style="display: none; text-align: center; color: #ff6b6b; font-style: italic; margin-top: 10px;">
        *NuPlan's results are not as good due to severe image distortion.
      </div>
  
      <div id="nuscenes-message" style="display: none; text-align: center; color: #ff6b6b; font-style: italic; margin-top: 10px;">
        *We interpolated the synchronized data to 10Hz (originally 2Hz), the interpolation artifacts contributes more visible noise.
      </div>
  
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="video-container">
            <video id="result-video" controls autoplay loop muted style="border-radius: 15px;">
              Your browser does not support the video tag.
            </video>
          </div>
        </div>
      </div> -->
    </div>
  </section>
  
  <section class="hero">
    <br><br><br>
    <div class="container is-max-desktop">
      <h2 class="title is-3">Abstract</h2>
      <div class="content">
        <p>
          Perception is crucial for autonomous driving, but single-agent perception is often constrained by sensors' physical limitations, leading to degraded performance under severe occlusion, adverse weather conditions, and when detecting distant objects. Multi-agent collaborative perception offers a solution, yet challenges arise when integrating heterogeneous agents with varying model architectures. To address these challenges, we propose STAMP, a scalable task- and model-agnostic, collaborative perception pipeline for heterogeneous agents. STAMP utilizes lightweight adapter-reverter pairs to transform Bird's Eye View (BEV) features between agent-specific and shared protocol domains, enabling efficient feature sharing and fusion. This approach minimizes computational overhead, enhances scalability, and preserves model security. Experiments on simulated and real-world datasets demonstrate STAMP's comparable or superior accuracy to state-of-the-art models with significantly reduced computational costs. As a first-of-its-kind task- and model-agnostic framework, STAMP aims to advance research in scalable and secure mobility systems towards Level 5 autonomy.
        </p>
      </div>
      
      <h2 class="title is-3">Pipeline Overview</h2>
      <div class="columns is-centered">
        <div class="column is-full">
          <img src="./static/images/pipeline.png" alt="method overview" class="image is-fullwidth">
        </div>
      </div>
      <div class="content">
        <p>STAMP allows agents to share feature maps in the protocal representation and keep the original local feature maps secure.</p>
        <ul>
          <li>We train a protocol model to learn a protocol feature space.</li>
          <li>All agents' adapters and reverters are trained locally</li>
          <li>Features of each local agent are adapted to the protocol feature space before being boardcast.</li>
          <li>Each local agent receives other agents' feature maps and reverts them to their own local feature spaces for fusion.</li>
        </ul>
      </div>

      <h2 class="title is-3">Training Efficiency</h2>
      <div class="columns is-centered">
        <div class="column is-full">
          <img src="./static/images/eff.png" alt="method overview" class="image is-fullwidth">
        </div>
      </div>
      <div class="content">
        <p>
          Training cost comparsion in the OPV2V dataset. in The changes in the number of training parameters and estimated training GPU hours as the number of heterogeneous agents increases from 1 to 12. End-to-end training and HEAL exhibit a steep increase in both parameters and GPU hours as the number of agents grows. In contrast, although our pipeline show higher parameters and GPU hours at the one or two number of agents (due to the training of the protocol model), it demonstrates a much slower growth rate because our proposed adapter reverter is very light-weighted and only takes 5 epochs to finish training. This highlights the scalability of our pipeline.
        </p>
      </div>

      <h2 class="title is-3">Performance in Task and Model-agnostic Collaborative Perception</h2>
      <div class="columns is-centered">
        <div class="column is-full">
          <img src="./static/images/stamp_cmp.png" alt="method overview" class="image is-fullwidth">
        </div>
      </div>
      <div class="content">
        <p>
          Heterogeneous collaborative perception results in a model- and task-agnostic setting. Tasks include 3D object detection ('Object Det'), static object BEV segmentation ('Static Seg'), and dynamic object BEV segmentation ('Dynamic Seg'). 3D object detection is evaluated using Average Precision at 50\% IoU threshold (AP@50), while segmentation tasks use Mean Intersection over Union (MIoU). Our method consistently outperforms single-agent segmentation for agents 3 and 4 in the BEV segmentation task. For agent 2's camera-based 3D object detection, our pipeline achieves substantial gains (e.g., AP@50 improves from 0.399 to 0.760 in noiseless conditions) while collaboration without feature alignment shows negligible changes.
        </p>
        <p>
          However, we observe that both collaborative approaches lead to performance degradation for Agent 1 compared to its single-agent baseline, despite our method outperforming collaboration without feature alignment. This unexpected outcome is attributed to Agent 2's limitations, which relies solely on less accurate camera sensors for 3D object detection. This scenario illustrates a bottleneck effect, where a weaker agent constrains the overall system performance, negatively impacting even the strongest agents. This challenge in multi-agent collaboration systems prompts us to introduce the concept of a \textbf{Multi-group Collaboration System}.
        </p>
      </div>
      

    </div>
    <br><br><br>
  </section>



  <section class="hero is-light method-comparison-section">
    <div class="hero-body">
      <div class="container is-widescreen">
        <h2 class="title is-3 has-text-centered">Our Vision</h2>

          <div class="columns is-centered">
            <div class="column is-full">
              <img src="./static/images/multi_group_sys.png" alt="method overview" class="image is-fullwidth">
            </div>
          </div>
          <div class="content">
          <p>In our experimental findings, we observed a bottleneck effect in collaborative perception systems, where the overall system performance is constrained by the capabilities of the weakest agent. This limitation underscores the need for more selective collaboration, leading us to introduce the concept of a <b>Collaboration Group</b> - a set of agents that collaborate under specific criteria. These criteria are essential for maintaining the quality and integrity of collaborative perception, admitting agents that meet predefined standards while excluding those with inferior models, potential malicious intent, or incompatible alignments.</p>

          <p>We can distinguish between three collaborative system types:</p>

          <ul>
              <li>Single-group systems, where agents either operate independently or are compelled to collaborate with all others, are susceptible to performance bottlenecks caused by inferior agents and vulnerabilities introduced by malicious attackers.</li>
              <li>Multi-group single-model systems, allowing multiple collaboration groups but restricting agents to a single group because each agent can only equip a single model.</li>
              <li>Multi-group multi-model systems, enabling agents to join multiple groups if they meet the predefined standards.</li>
          </ul>

          <p>The multi-group structure offers significant advantages over traditional single-group systems. It enhances agents' potential for diverse collaborations, consequently improving overall performance. This approach mitigates the bottleneck effect by allowing high-performing agents to maintain efficiency within groups of similar capability while potentially assisting less capable agents in other groups. Furthermore, it enhances system flexibility, enabling dynamic group formation based on specific task requirements or environmental conditions.</p>

          <p>However, implementing such a multi-group system poses challenges for existing heterogeneous collaborative pipelines. End-to-end training approaches require simultaneous training of all models, conflicting with the concept of distinct collaboration groups. Methods that require separate encoders for each group become impractical as the number of groups increases due to computational and memory constraints.</p>

          <p>Our proposed STAMP framework effectively addresses these limitations, offering a scalable solution for multi-group collaborative perception. The key innovation lies in its lightweight adapter and reverter pair (approximately 1MB) required for each collaboration group an agent joins. This efficient design enables agents to equip multiple adapter-reverter pairs, facilitating seamless participation in various groups without significant computational overhead. The minimal memory footprint ensures scalability, even as agents join numerous collaboration groups, making STAMP particularly well-suited for multi-group and multi-model collaboration systems.</p>
          </div>

          <div id="comparison-controls" style="margin-top: 20px; text-align: center;">
            <div class="buttons is-centered">
              <!-- Buttons will be dynamically added here -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  